import os
import json
import hashlib
import math
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
import secrets
import subprocess
import platform
import re

import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# =========================================================
# Lilot (Embedding-only RAG)
# - Embeddingæ¤œç´¢ã®ã¿ï¼ˆKeywordæ¤œç´¢ãªã—ï¼‰
# - Embedding: ãƒªãƒ¢ãƒ¼ãƒˆAPI(OpenAIäº’æ›) or ãƒ­ãƒ¼ã‚«ãƒ«MiniLM
# - æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆtxt/csv/pdfï¼‰ã‚’ UI ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ data/uploads ã«ä¿å­˜
# - PDFã¯ãƒšãƒ¼ã‚¸â†’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆå·¨å¤§ãƒ–ãƒ­ãƒƒã‚¯å›é¿ï¼‰
# - å‚ç…§ãƒŠãƒ¬ãƒƒã‚¸ã« Source è¡¨ç¤º
# - å–å¾—çµæœã« uploads ç”±æ¥ã‚’æœ€ä½1ä»¶å«ã‚ã‚‹ï¼ˆuploadsãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
# =========================================================

# ---------------------------------------------------------
# .env èª­ã¿è¾¼ã¿ & ç’°å¢ƒå¤‰æ•°
# ---------------------------------------------------------
load_dotenv()

LLM_API_KEY = os.getenv("LOCALLM_API_KEY") or os.getenv("LLM_API_KEY")
LLM_BASE_URL = os.getenv("LOCALLM_BASE_URL") or os.getenv("LLM_BASE_URL", "")
LLM_MODEL = os.getenv("LOCALLM_CHAT_MODEL") or os.getenv("LLM_MODEL", "")

EMB_API_KEY = os.getenv("EMB_API_KEY")
EMB_BASE_URL = os.getenv("EMB_BASE_URL", "https://api.openai.com/v1")
EMB_MODEL = os.getenv("EMB_MODEL", "text-embedding-3-small")

LOCAL_EMB_MODEL_PATH = os.getenv(
    "LOCAL_EMB_MODEL_PATH",
    "sentence-transformers/all-MiniLM-L6-v2",
)

def use_remote_embedding() -> bool:
    return bool(EMB_API_KEY)

# ---------------------------------------------------------
# ãƒ‘ã‚¹è¨­å®š
# ---------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parents[1]
DATA_DIR = BASE_DIR / "data"
LOGS_DIR = BASE_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

UPLOAD_DIR = DATA_DIR / "uploads"
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# å®‰å®šç‰ˆ: PDFã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«æŠ½å‡ºã—ã¦ä¿å­˜ã—ã€èµ·å‹•æ™‚ã¯æŠ½å‡ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‚ç…§ã™ã‚‹
UPLOAD_ORIGINAL_DIR = UPLOAD_DIR / "original"
UPLOAD_EXTRACTED_DIR = UPLOAD_DIR / "extracted"
UPLOAD_ORIGINAL_DIR.mkdir(parents=True, exist_ok=True)
UPLOAD_EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)
UPLOAD_META_PATH = UPLOAD_DIR / "index_meta.json"

# â˜…â˜… ãƒ­ã‚´ã¨ãƒ•ã‚¡ãƒ“ã‚³ãƒ³ã®çµ¶å¯¾ãƒ‘ã‚¹
LOGO_PATH = (BASE_DIR / "lilot.png").resolve()
FAVICON_PATH = (BASE_DIR / "lilot_mark.png").resolve()
ENV_PATH = (BASE_DIR / ".env").resolve()

# ---------------------------------------------------------
# æ·»ä»˜ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# ---------------------------------------------------------
ALLOWED_UPLOAD_EXTS = {".txt", ".csv", ".pdf"}

def _safe_filename(name: str) -> str:
    name = (name or "").strip()
    # Windows NGæ–‡å­—
    name = re.sub(r"[\\/:*?\"<>|]+", "_", name)
    name = re.sub(r"\s+", " ", name).strip()
    return name[:200] if name else "uploaded_file"

def save_uploaded_files(files) -> dict:
    """
    ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã€PDFã¯ä¿å­˜ç›´å¾Œã«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã—ã¦ extracted ã«ä¿å­˜ã—ã¾ã™ï¼ˆè¤‡æ•°åŒæ™‚å¯¾å¿œï¼‰ã€‚
    è¿”ã‚Šå€¤:
      {
        "saved": [Path...],         # ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆtxt/csv/pdfï¼‰
        "pdf_extracted": [str...],  # æŠ½å‡ºã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«å
        "pdf_skipped": [str...],    # å¤‰æ›´ãªã—ã§ã‚¹ã‚­ãƒƒãƒ—ã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«å
        "pdf_failed": [str...],     # æŠ½å‡ºå¤±æ•—ã—ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«å
      }
    """
    result = {"saved": [], "pdf_extracted": [], "pdf_skipped": [], "pdf_failed": []}
    if not files:
        return result

    meta = load_upload_meta()

    for f in files:
        try:
            name = os.path.basename(getattr(f, "name", ""))
            if not name:
                continue
            ext = os.path.splitext(name)[1].lower()

            data = f.getvalue() if hasattr(f, "getvalue") else f.read()
            if not isinstance(data, (bytes, bytearray)):
                continue

            # txt/csv ã¯ uploads/ ç›´ä¸‹ã«ä¿å­˜ï¼ˆè»½ã„ï¼‰
            if ext in [".txt", ".csv"]:
                out = UPLOAD_DIR / name
                out.write_bytes(data)
                result["saved"].append(out)
                continue

            # pdf ã¯ original/ ã«ä¿å­˜ã—ã€å¿…è¦ãªã‚‰æŠ½å‡º
            if ext == ".pdf":
                out_pdf = UPLOAD_ORIGINAL_DIR / name
                out_pdf.write_bytes(data)
                result["saved"].append(out_pdf)

                sha = _sha256_bytes(data)
                prev = meta.get(name, {})
                if prev.get("sha256") == sha and extracted_json_path_for(name).exists():
                    result["pdf_skipped"].append(name)
                    continue

                                # æŠ½å‡ºã—ã¦ä¿å­˜ï¼ˆã“ã“ã§ãƒãƒ£ãƒ³ã‚¯åŒ–ã¾ã§å®Œäº†ã•ã›ã€èµ·å‹•æ™‚ã«é‡å‡¦ç†ã‚’ã—ãªã„ï¼‰
                pages = _extract_pdf_pages(out_pdf, max_pages=80, max_chars_per_page=8000, max_total_chars=200000)

                chunks: List[Dict[str, str]] = []
                total_chars = 0
                for page_idx, page_text in enumerate(pages, 1):
                    if not (page_text or "").strip():
                        continue
                    # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®ãƒãƒ£ãƒ³ã‚¯æ•°ã‚‚åˆ¶é™ï¼ˆæš´ç™ºé˜²æ­¢ï¼‰
                    parts = chunk_text(page_text, max_chars=900, overlap=180, hard_char_limit=12000, max_chunks=40)
                    for cidx, part in enumerate(parts, 1):
                        chunks.append({"text": part, "source": f"uploads/{name}#p{page_idx}-c{cidx}"})
                        total_chars += len(part)
                    # PDF 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šé™
                    if len(chunks) >= 1200:
                        break
                chunks = chunks[:1200]

                if total_chars <= 0 or not chunks:
                    meta[name] = {
                        "sha256": sha,
                        "status": "no_text",
                        "pages": len(pages),
                        "chars": 0,
                        "chunks": 0,
                    }
                    result["pdf_failed"].append(name)
                    continue

                extracted_path = extracted_json_path_for(name)
                payload = {"chunks": chunks, "pages_count": len(pages), "chars": total_chars}
                extracted_path.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")

                meta[name] = {
                    "sha256": sha,
                    "status": "ready",
                    "pages": len(pages),
                    "chars": total_chars,
                    "chunks": len(chunks),
                }
                result["pdf_extracted"].append(name)
                continue

        except Exception:
            continue

    save_upload_meta(meta)
    return result

def invalidate_knowledge_cache():
    """
    uploads ã«å¤‰æ›´ãŒå…¥ã£ãŸã¨ãã«ã€ãƒŠãƒ¬ãƒƒã‚¸èª­è¾¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç¢ºå®Ÿã«ç ´æ£„ã—ã¾ã™ã€‚
    Streamlit ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å·®ã§ .clear() ãŒåŠ¹ã‹ãªã„ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€ä¸¡æ–¹è©¦ã—ã¾ã™ã€‚
    """
    # å€‹åˆ¥ã‚¯ãƒªã‚¢ï¼ˆæ–°ã—ã‚ã®Streamlitï¼‰
    for fn in (load_knowledge, build_corpus_index, _pdf_extract_summary):
        try:
            fn.clear()  # type: ignore[attr-defined]
        except Exception:
            pass

    # å…¨ä½“ã‚¯ãƒªã‚¢ï¼ˆäº’æ›æ€§é‡è¦–ï¼‰
    try:
        st.cache_data.clear()
    except Exception:
        pass
    try:
        st.cache_resource.clear()
    except Exception:
        pass


# ---------------------------------------------------------
# ãƒ­ã‚°é–¢é€£
# ---------------------------------------------------------
def _get_log_path() -> Path:
    date_str = datetime.now().strftime("%Y%m%d")
    session_id = getattr(st.session_state, "session_id", None) or "default"
    return LOGS_DIR / f"{date_str}_{session_id}.jsonl"

def log_interaction(question: str, answer: str, contexts: List[Dict[str, Any]], extra=None):
    extra = extra or {}
    rec = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer,
        "contexts": contexts,  # [{"source":..., "score":..., "text":...}, ...]
    }
    rec.update(extra)
    try:
        with _get_log_path().open("a", encoding="utf-8") as f:
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    except Exception:
        pass

def list_log_files() -> List[Path]:
    return sorted(LOGS_DIR.glob("*.jsonl"), reverse=True)

def load_history_from_log(log_path: Path):
    history = []
    if not log_path.exists():
        return history
    for line in log_path.open("r", encoding="utf-8"):
        line = line.strip()
        if not line:
            continue
        try:
            rec = json.loads(line)
        except Exception:
            continue
        q = rec.get("question")
        a = rec.get("answer")
        if q and a:
            history.append({"user": q, "assistant": a})
    return history

# ---------------------------------------------------------
# LLM / Embedding ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ---------------------------------------------------------
def get_llm_client():
    if not LLM_API_KEY:
        return "LLM_API_KEY ãŒã‚ã‚Šã¾ã›ã‚“"
    return OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

def get_emb_client():
    if not EMB_API_KEY:
        return "EMB_API_KEY ãŒã‚ã‚Šã¾ã›ã‚“"
    return OpenAI(api_key=EMB_API_KEY, base_url=EMB_BASE_URL)

# ---------------------------------------------------------
# ãƒ­ãƒ¼ã‚«ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«
# ---------------------------------------------------------
@st.cache_resource(show_spinner=True)
def load_local_embedder():
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError as e:
        raise RuntimeError(
            "sentence-transformers ãŒå¿…è¦ã§ã™ã€‚\n"
            "pip install sentence-transformers ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"
        ) from e
    return SentenceTransformer(LOCAL_EMB_MODEL_PATH)

# ---------------------------------------------------------
# system_prompt èª­ã¿è¾¼ã¿
# ---------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_system_prompt():
    path = DATA_DIR / "system_prompt.txt"
    if path.exists():
        t = path.read_text(encoding="utf-8", errors="ignore").strip()
        if t:
            return t
    return (
        "ã‚ãªãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã‚’æ´»ç”¨ã™ã‚‹ç¤¾å†…ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯AIã§ã™ã€‚\n"
        "å¸¸ã«æ—¥æœ¬èªã§ä¸å¯§ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    )

# ---------------------------------------------------------
# ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
# ---------------------------------------------------------
def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = s.replace("\r\n", "\n").replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def chunk_text(text: str, max_chars: int = 900, overlap: int = 150,
               hard_char_limit: int = 20000, max_chunks: int = 200) -> List[str]:
    """
    æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆå·¨å¤§ãƒ–ãƒ­ãƒƒã‚¯å¯¾ç­–ãƒ»å®‰å…¨ã‚¬ãƒ¼ãƒ‰ä»˜ãï¼‰
    - hard_char_limit ã‚’è¶…ãˆã‚‹å…¥åŠ›ã¯å…ˆé ­ã®ã¿ã§æ‰“ã¡åˆ‡ã‚Šï¼ˆMemoryErroré˜²æ­¢ï¼‰
    - max_chunks ã‚’è¶…ãˆãŸã‚‰æ‰“ã¡åˆ‡ã‚Šï¼ˆæš´ç™ºé˜²æ­¢ï¼‰
    """
    text = normalize_text(text)
    if not text:
        return []

    if hard_char_limit and len(text) > hard_char_limit:
        text = text[:hard_char_limit]

    paras = [p.strip() for p in text.split("\n\n") if p.strip()]
    if not paras:
        paras = [text]

    chunks: List[str] = []
    buf: List[str] = []
    buf_len = 0

    def flush():
        nonlocal buf, buf_len
        if not buf:
            return
        s = "\n\n".join(buf).strip()
        if s:
            chunks.append(s)
        buf = []
        buf_len = 0

    for p in paras:
        if max_chunks and len(chunks) >= max_chunks:
            break

        extra = 2 if buf else 0
        if buf_len + len(p) + extra <= max_chars:
            buf.append(p)
            buf_len += len(p) + extra
        else:
            flush()
            if max_chunks and len(chunks) >= max_chunks:
                break

            if len(p) > max_chars:
                start = 0
                while start < len(p):
                    if max_chunks and len(chunks) >= max_chunks:
                        break
                    end = min(start + max_chars, len(p))
                    part = p[start:end].strip()
                    if part:
                        chunks.append(part)
                    start = max(0, end - overlap)
                    if start == end:
                        break
            else:
                buf = [p]
                buf_len = len(p)

    flush()
    out = [c for c in chunks if c.strip()]
    if max_chunks:
        out = out[:max_chunks]
    return out

# ---------------------------------------------------------
# PDF æŠ½å‡ºï¼ˆpypdfï¼‰
# ---------------------------------------------------------
def _pdf_can_extract_text() -> bool:
    try:
        import pypdf  # noqa: F401
        return True
    except Exception:
        return False

def _extract_pdf_pages(path: Path, max_pages: int = 50,
                       max_chars_per_page: int = 8000,
                       max_total_chars: int = 200000) -> List[str]:
    """
    PDFã‚’ãƒšãƒ¼ã‚¸å˜ä½ã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã—ã¾ã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½¿ã„ã¾ã›ã‚“ï¼‰ã€‚
    â€» æŠ½å‡ºã¯ã€Œã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç›´å¾Œã€ã«ã®ã¿è¡Œã„ã€çµæœã¯ uploads/extracted/*.json ã«ä¿å­˜ã—ã¾ã™ã€‚
    å®‰å…¨ã‚¬ãƒ¼ãƒ‰:
      - 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Š max_chars_per_page ã§æ‰“ã¡åˆ‡ã‚Š
      - PDFå…¨ä½“ã§ max_total_chars ã§æ‰“ã¡åˆ‡ã‚Šï¼ˆå·¨å¤§PDFã§ã®ãƒ¡ãƒ¢ãƒªæš´ç™ºé˜²æ­¢ï¼‰
    """
    try:
        from pypdf import PdfReader
    except Exception:
        return []

    try:
        reader = PdfReader(str(path))
        pages: List[str] = []
        total = 0
        for i, page in enumerate(reader.pages):
            if i >= max_pages:
                break
            txt = page.extract_text() or ""
            txt = normalize_text(txt)
            if max_chars_per_page and len(txt) > max_chars_per_page:
                txt = txt[:max_chars_per_page]
            pages.append(txt)
            total += len(txt)
            if max_total_chars and total >= max_total_chars:
                break
        return pages
    except Exception:
        return []

    try:
        reader = PdfReader(str(path))
        pages: List[str] = []
        for i, page in enumerate(reader.pages):
            if i >= max_pages:
                break
            txt = page.extract_text() or ""
            pages.append(normalize_text(txt))
        return pages
    except Exception:
        return []


@st.cache_data(show_spinner=False)
def _pdf_extract_summary(path_str: str, mtime: float):
    p = Path(path_str)
    pages = _extract_pdf_pages(p, max_pages=50)
    txt = "\n\n".join([t for t in pages if t])
    return {
        "chars": len(txt),
        "pages": len(pages),
        "sample": (txt[:160] + "â€¦") if len(txt) > 160 else txt,
    }

# ---------------------------------------------------------
# Knowledge èª­ã¿è¾¼ã¿ï¼ˆtxt/csv/pdfï¼‰ with source
# ---------------------------------------------------------
def _append_chunked_docs(docs: List[Dict[str, str]], text: str, source_prefix: str,
                         max_chars: int = 900, overlap: int = 150):
    parts = chunk_text(text, max_chars=max_chars, overlap=overlap)
    for i, part in enumerate(parts, 1):
        docs.append({"text": part, "source": f"{source_prefix}#c{i}"})

@st.cache_data(show_spinner=False)
def load_knowledge() -> List[Dict[str, str]]:
    """
    èµ·å‹•æ™‚ã«è»½ãå‹•ã‹ã™ãŸã‚ã€PDFæœ¬ä½“ã¯èª­ã¾ãšã€æŠ½å‡ºæ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ï¼ˆuploads/extracted/*.jsonï¼‰ã ã‘ã‚’èª­ã¿ã¾ã™ã€‚
    å¯¾è±¡:
      - data/knowledge.txt
      - data/uploads/*.txt, *.csv
      - data/uploads/extracted/*.json  (PDFæŠ½å‡ºæ¸ˆã¿: chunks)
    """
    docs: List[Dict[str, str]] = []

    # main knowledge
    kp = DATA_DIR / "knowledge.txt"
    if kp.exists():
        t = kp.read_text(encoding="utf-8", errors="ignore")
        _append_chunked_docs(docs, t, "knowledge.txt")

    # uploads: txt
    for p in UPLOAD_DIR.glob("*.txt"):
        try:
            t = p.read_text(encoding="utf-8", errors="ignore")
            _append_chunked_docs(docs, t, f"uploads/{p.name}")
        except Exception:
            continue

    # uploads: csvï¼ˆ1è¡Œï¼1ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
    import csv
    for p in UPLOAD_DIR.glob("*.csv"):
        try:
            with p.open("r", encoding="utf-8", errors="ignore", newline="") as f:
                reader = csv.reader(f)
                next(reader, None)
                for ridx, row in enumerate(reader, 1):
                    srow = ", ".join(col.strip() for col in row if col and col.strip())
                    if not srow:
                        continue
                    parts = chunk_text(srow, max_chars=900, overlap=120, hard_char_limit=6000, max_chunks=10)
                    for cidx, part in enumerate(parts, 1):
                        docs.append({"text": part, "source": f"uploads/{p.name}#row{ridx}-c{cidx}"})
        except Exception:
            continue

    # extracted: pdf chunks jsonï¼ˆèµ·å‹•æ™‚ã¯ãƒãƒ£ãƒ³ã‚¯åŒ–æ¸ˆã¿ã‚’èª­ã‚€ã ã‘ï¼‰
    for jp in UPLOAD_EXTRACTED_DIR.glob("*.json"):
        try:
            data = json.loads(jp.read_text(encoding="utf-8", errors="ignore"))
        except Exception:
            continue

        chunks = None
        if isinstance(data, dict) and isinstance(data.get("chunks"), list):
            chunks = data.get("chunks")
        # å¾Œæ–¹äº’æ›: pageså½¢å¼ãŒæ®‹ã£ã¦ã„ãŸå ´åˆã¯ã€èµ·å‹•æ™‚ã«æš´ç™ºã—ãªã„ã‚ˆã†æ¥µå°ã§å–ã‚Šè¾¼ã¿
        elif isinstance(data, dict) and isinstance(data.get("pages"), list):
            pages = data.get("pages") or []
            pdf_name = jp.name[:-5]
            for page_idx, page_text in enumerate(pages[:3], 1):  # å…ˆé ­æ•°ãƒšãƒ¼ã‚¸ã ã‘
                if not (page_text or "").strip():
                    continue
                parts = chunk_text(page_text, max_chars=900, overlap=180, hard_char_limit=6000, max_chunks=10)
                for cidx, part in enumerate(parts, 1):
                    docs.append({"text": part, "source": f"uploads/{pdf_name}#p{page_idx}-c{cidx}"})
            continue
        else:
            continue

        for c in chunks:
            try:
                text = (c.get("text") if isinstance(c, dict) else "") or ""
                source = (c.get("source") if isinstance(c, dict) else "") or ""
                if text.strip():
                    docs.append({"text": text, "source": source or f"uploads/{jp.name}"})
            except Exception:
                continue

    return docs


def get_knowledge_docs() -> List[Dict[str, str]]:
    return load_knowledge()

# ---------------------------------------------------------
# ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
# ---------------------------------------------------------
def init_session_state():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "history" not in st.session_state:
        st.session_state.history = []
    if "loaded_log_name" not in st.session_state:
        st.session_state.loaded_log_name = None
    if "session_id" not in st.session_state:
        st.session_state.session_id = secrets.token_hex(6)


def add_history(user, assistant):
    st.session_state.history.append({"user": user, "assistant": assistant})
    st.session_state.messages.append({"role": "user", "content": user})
    st.session_state.messages.append({"role": "assistant", "content": assistant})

def get_history():
    return st.session_state.history


# ---------------------------------------------------------
# ã‚¯ã‚¨ãƒªã®æ´¾ç”Ÿï¼ˆEmbeddingã®ã¿ã§æ‹¾ã„ã‚„ã™ãã™ã‚‹ï¼‰
# ---------------------------------------------------------
def extract_title_hint(query: str) -> str:
    """
    ä¾‹:
      æœ¬è«–æ–‡=TITLE ã®è‘—è€… â†’ TITLE ã‚’æŠ½å‡º
      ã€ŒTITLEã€ã®è‘—è€… â†’ TITLE ã‚’æŠ½å‡º
    """
    q = (query or "").strip()

    m = re.search(r"[=ï¼]\s*([A-Za-z0-9][^\n]+?)\s*(ã®è‘—è€…|ã®ç­†è€…|è‘—è€…|author|authors)\b", q, flags=re.IGNORECASE)
    if m:
        return m.group(1).strip()

    m = re.search(r"[ã€Œã€\"](.+?)[ã€ã€\"]\s*(ã®è‘—è€…|ã®ç­†è€…|è‘—è€…|author|authors)\b", q, flags=re.IGNORECASE)
    if m:
        return m.group(1).strip()

    m = re.search(r"(.+?)\s*(ã®è‘—è€…|ã®ç­†è€…|è‘—è€…)\b", q)
    if m:
        cand = m.group(1).strip()
        return cand[-160:].strip() if len(cand) > 160 else cand

    return ""

def build_query_variants(query: str) -> List[str]:
    q = (query or "").strip()
    title = extract_title_hint(q)
    vars = [q]
    if title and title != q:
        vars.extend([
            title,
            f"authors of {title}",
            f"{title} authors",
            f"{title} author list",
            f"{title} è‘—è€…",
        ])
    out = []
    seen = set()
    for x in vars:
        x = (x or "").strip()
        if not x:
            continue
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out

# ---------------------------------------------------------
# embedding
# ---------------------------------------------------------
def cosine_similarity(a, b):
    if not a or not b:
        return 0.0
    n = min(len(a), len(b))
    dot = sum(a[i] * b[i] for i in range(n))
    na = math.sqrt(sum(a[i] * a[i] for i in range(n)))
    nb = math.sqrt(sum(b[i] * b[i] for i in range(n)))
    return dot / (na * nb) if na and nb else 0.0

def embed_texts(texts: List[str]):
    if use_remote_embedding():
        c = get_emb_client()
        if isinstance(c, str):
            raise RuntimeError(c)
        resp = c.embeddings.create(model=EMB_MODEL, input=texts)
        return [d.embedding for d in resp.data]
    else:
        enc = load_local_embedder()
        arr = enc.encode(texts, show_progress_bar=False)
        return [list(map(float, v)) for v in arr]

@st.cache_resource(show_spinner=True)
def build_corpus_index():
    docs = get_knowledge_docs()  # [{"text":..., "source":...}, ...]
    if not docs:
        return [], []
    texts = [d["text"] for d in docs]
    vecs = embed_texts(texts)
    return docs, vecs

def _is_upload_source(src: str) -> bool:
    return (src or "").startswith("uploads/")

def retrieve_with_embedding(query: str, top_k: int = 3, min_uploads: int = 1, use_variants: bool = False) -> List[Dict[str, Any]]:
    """
    å–å¾—çµæœã« uploads ç”±æ¥ã‚’æœ€ä½ min_uploads ä»¶å«ã‚ã‚‹ï¼ˆuploadsãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
    - æ¤œç´¢æ–¹å¼ï¼ˆEmbeddingï¼‰ã¯ç¶­æŒ
    - use_variants=True ã®å ´åˆã€æ´¾ç”Ÿã‚¯ã‚¨ãƒªã‚’è¤‡æ•°ä½œã£ã¦ã€Œæœ€å¤§é¡ä¼¼åº¦ã€ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆã‚¿ã‚¤ãƒˆãƒ«â†’è‘—è€…è¡Œã‚’æ‹¾ã„ã‚„ã™ãï¼‰
    """
    docs, vecs = build_corpus_index()
    if not docs or not vecs:
        return []

    queries = build_query_variants(query) if use_variants else [query]
    q_vecs = embed_texts(queries)

    scored: List[Dict[str, Any]] = []
    for d, v in zip(docs, vecs):
        s = max(cosine_similarity(qv, v) for qv in q_vecs) if q_vecs else 0.0
        scored.append({
            "score": float(s),
            "text": d.get("text", ""),
            "source": d.get("source", "unknown"),
        })

    scored.sort(key=lambda x: x["score"], reverse=True)
    if top_k <= 0:
        return []

    has_uploads = any(_is_upload_source(x["source"]) for x in scored)
    need_uploads = min_uploads if has_uploads else 0
    need_uploads = max(0, min(need_uploads, top_k))

    selected: List[Dict[str, Any]] = []
    used_sources = set()

    if need_uploads > 0:
        for x in scored:
            if _is_upload_source(x["source"]) and x["source"] not in used_sources:
                selected.append(x)
                used_sources.add(x["source"])
                if len(selected) >= need_uploads:
                    break

    for x in scored:
        if len(selected) >= top_k:
            break
        if x["source"] in used_sources:
            continue
        selected.append(x)
        used_sources.add(x["source"])

    if len(selected) < top_k:
        for x in scored:
            if len(selected) >= top_k:
                break
            selected.append(x)

    return selected[:top_k]

# ---------------------------------------------------------
# LLM å‘¼ã³å‡ºã—
# ---------------------------------------------------------
def call_llm_with_context(query: str, contexts: List[Dict[str, Any]]) -> str:
    client = get_llm_client()
    if isinstance(client, str):
        return client

    hist = get_history()

    if contexts:
        ctx_text = "\n\n---\n\n".join(
            [f"[{c.get('source','unknown')}]\n{c.get('text','')}" for c in contexts]
        )
    else:
        ctx_text = "ãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    sys_base = load_system_prompt()
    sys_content = f"{sys_base}\n\nã€é‡è¦ã€‘ä»¥ä¸‹ã®ã€ŒæŠ½å‡ºã•ã‚ŒãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã€ä»¥å¤–ã‚’æ ¹æ‹ ã«æ¨æ¸¬ãƒ»è£œå®Œã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ç­”ãˆãŒãƒŠãƒ¬ãƒƒã‚¸å†…ã«ç„¡ã„å ´åˆã¯ã€å¿…ãšã€ŒãƒŠãƒ¬ãƒƒã‚¸å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨ã ã‘å›ç­”ã—ã¦ãã ã•ã„ã€‚å¤–éƒ¨ã‚µã‚¤ãƒˆæ¤œç´¢ã®ææ¡ˆã‚„ä¸€èˆ¬è«–ã®èª¬æ˜ã‚‚ä¸è¦ã§ã™ã€‚\n\n-----\nä»¥ä¸‹ã¯æŠ½å‡ºã•ã‚ŒãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸ã§ã™ï¼š\n{ctx_text}"

    msgs = [{"role": "system", "content": sys_content}]
    for h in hist[-5:]:
        msgs.append({"role": "user", "content": h["user"]})
        msgs.append({"role": "assistant", "content": h["assistant"]})
    msgs.append({"role": "user", "content": query})

    resp = client.chat.completions.create(
        model=LLM_MODEL,
        messages=msgs,
        temperature=0.0,
    )
    return resp.choices[0].message.content or ""

# ---------------------------------------------------------
# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãï¼ˆãƒ¡ãƒ¢å¸³ / ãƒ•ã‚©ãƒ«ãƒ€ï¼‰
# ---------------------------------------------------------
def open_with_notepad(path: Path):
    try:
        if platform.system() == "Windows":
            subprocess.Popen(["notepad.exe", str(path)])
        else:
            # mac/linux ã¯æ—¢å®šã‚¢ãƒ—ãƒª
            if platform.system() == "Darwin":
                subprocess.Popen(["open", str(path)])
            else:
                subprocess.Popen(["xdg-open", str(path)])
    except Exception:
        pass

def open_in_file_manager(path: Path):
    try:
        if platform.system() == "Windows":
            subprocess.Popen(["explorer", str(path)])
        else:
            if platform.system() == "Darwin":
                subprocess.Popen(["open", str(path)])
            else:
                subprocess.Popen(["xdg-open", str(path)])
    except Exception:
        pass


def _sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def load_upload_meta() -> dict:
    if UPLOAD_META_PATH.exists():
        try:
            return json.loads(UPLOAD_META_PATH.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def save_upload_meta(meta: dict) -> None:
    try:
        UPLOAD_META_PATH.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass

def extracted_json_path_for(pdf_name: str) -> Path:
    return UPLOAD_EXTRACTED_DIR / f"{pdf_name}.json"


# ---------------------------------------------------------
# ãƒ­ã‚° â†’ ãƒãƒ£ãƒƒãƒˆæŠ•å…¥ï¼ˆæœ€æ–°1ä»¶ï¼‰
# ---------------------------------------------------------
def load_latest_log_into_chat():
    logs = list_log_files()
    if not logs:
        return False
    latest = logs[0]
    history = load_history_from_log(latest)
    if not history:
        return False
    st.session_state.history = history[:]
    st.session_state.messages = []
    for h in history:
        st.session_state.messages.append({"role": "user", "content": h["user"]})
        st.session_state.messages.append({"role": "assistant", "content": h["assistant"]})
    st.session_state.loaded_log_name = latest.name
    return True


def load_log_into_chat(log_path: Path):
    history = load_history_from_log(log_path)
    if not history:
        return False
    st.session_state.history = history[:]
    st.session_state.messages = []
    for h in history:
        st.session_state.messages.append({"role": "user", "content": h["user"]})
        st.session_state.messages.append({"role": "assistant", "content": h["assistant"]})
    st.session_state.loaded_log_name = log_path.name
    return True


# ---------------------------------------------------------
# Streamlit UI
# ---------------------------------------------------------
def main():
    st.set_page_config(
        page_title="Lilot",
        page_icon=str(FAVICON_PATH),
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.markdown("### ğŸ” Lilot AIã¸ã®ä¾é ¼ã‚„è³ªå•ã‚’å…¥åŠ›ã—ã¦é€ä¿¡ã—ã¦ä¸‹ã•ã„ã€‚")
    st.caption("ãŠå¾…ãŸã›ã—éããŸAIãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€‚ç°¡å˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€ãƒ­ãƒ¼ã‚«ãƒ«ç¨¼åƒãŒã§ãã¾ã™ã€‚Knowledge.txtã§RAGãŒå‡ºæ¥ã¾ã™ã€‚")

    init_session_state()
    docs_all = get_knowledge_docs()
    doc_count = len(docs_all)
    upload_count = sum(1 for d in docs_all if str(d.get("source","")).startswith("uploads/"))
    st.caption(f"Index: {doc_count} chunks (uploads: {upload_count})")

    # -----------------------------------------------------
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    # -----------------------------------------------------
    with st.sidebar:

        # â˜…â˜… ãƒ­ã‚´ã‚’ã‚»ãƒ³ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã€ŒåŠåˆ†ã®å¤§ãã•ï¼ˆ50pxï¼‰ã€ã§è¡¨ç¤º
        col_l, col_c, col_r = st.columns([1, 2, 1])
        with col_c:
            if LOGO_PATH.exists():
                st.image(str(LOGO_PATH), width=50)

        st.markdown("### ğŸ” Lilot")
        st.caption("Light-weight local AI chat application")

        if st.button("ğŸ†• æ–°è¦ãƒãƒ£ãƒƒãƒˆ", use_container_width=True):
            st.session_state.history = []
            st.session_state.messages = []
            st.session_state.loaded_log_name = None
            st.success("æ–°ã—ã„ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
            st.rerun()

        with st.expander("ğŸ“„ ãƒŠãƒ¬ãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
            st.caption("knowledge.txt / system_prompt.txt ã‚’é–‹ãã¾ã™ï¼ˆç·¨é›†ã¯è‡ªå·±è²¬ä»»ã§ãŠé¡˜ã„ã—ã¾ã™ï¼‰ã€‚")
            cka, ckb = st.columns(2)
            with cka:
                if st.button("knowledge.txt ã‚’é–‹ã", use_container_width=True):
                    open_with_notepad(DATA_DIR / "knowledge.txt")
            with ckb:
                if st.button("system_prompt.txt ã‚’é–‹ã", use_container_width=True):
                    open_with_notepad(DATA_DIR / "system_prompt.txt")

        
        with st.expander("ğŸ§¾ ãƒ­ã‚°", expanded=False):
            st.caption("logs/ ã«ä¿å­˜ã•ã‚ŒãŸä¼šè©±ãƒ­ã‚°ï¼ˆjsonlï¼‰ã‚’ç¢ºèªã§ãã¾ã™ã€‚")

            if st.button("logs ã‚’é–‹ã", use_container_width=True, key="open_logs_btn"):
                open_in_file_manager(LOGS_DIR)

            logs = list_log_files()
            if logs:
                st.caption("æœ€è¿‘ã®ãƒ­ã‚°ï¼ˆâ†’ ã§ãƒ­ãƒ¼ãƒ‰ï¼‰")
                for lp in logs[:10]:
                    c1, c2 = st.columns([6, 1])
                    with c1:
                        st.write(lp.name)
                    with c2:
                        if st.button("â†’", key=f"log_arrow_{lp.name}"):
                            ok = load_log_into_chat(lp)
                            if ok:
                                st.success(f"{lp.name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
                                st.rerun()
                            else:
                                st.warning("ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.caption("ãƒ­ã‚°ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")


        st.markdown("---")

        # â˜…â˜… æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè¿½åŠ ï¼‰
        with st.expander("ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«", expanded=False):
            st.caption("txt/csv ã¯ä¿å­˜å¾Œã™ãæ¤œç´¢å¯¾è±¡ã«ãªã‚Šã¾ã™ã€‚PDF ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç›´å¾Œã«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã—ã€ä»¥é™ã¯å·®åˆ†ãŒç„¡ã„é™ã‚Šå†æŠ½å‡ºã—ã¾ã›ã‚“ã€‚")

            files = st.file_uploader("txt / csv / pdfï¼ˆè¤‡æ•°å¯ï¼‰", type=["txt", "csv", "pdf"], accept_multiple_files=True)

            c1, c2 = st.columns([1, 1])
            with c1:
                if st.button("ä¿å­˜ã—ã¦å–ã‚Šè¾¼ã‚€", use_container_width=True) and files:
                    with st.spinner("ä¿å­˜ãƒ»å–ã‚Šè¾¼ã¿ä¸­...ï¼ˆPDFã¯åˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
                        res = save_uploaded_files(files)

                    if res.get("saved"):
                        invalidate_knowledge_cache()
                        msg = []
                        msg.append("ä¿å­˜: " + ", ".join([Path(p).name for p in res["saved"]]))
                        if res.get("pdf_extracted"):
                            msg.append("PDFæŠ½å‡º: " + ", ".join(res["pdf_extracted"]))
                        if res.get("pdf_skipped"):
                            msg.append("PDFã‚¹ã‚­ãƒƒãƒ—(å¤‰æ›´ãªã—): " + ", ".join(res["pdf_skipped"]))
                        if res.get("pdf_failed"):
                            msg.append("PDFæŠ½å‡ºå¤±æ•—: " + ", ".join(res["pdf_failed"]))
                        st.success("\n".join(msg))
                        st.rerun()
                    else:
                        st.warning("ä¿å­˜ã§ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            with c2:
                if st.button("uploads ã‚’é–‹ã", use_container_width=True):
                    open_in_file_manager(UPLOAD_DIR)

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºï¼ˆé‡ã„å‡¦ç†ã¯ã—ãªã„ï¼‰
            meta = load_upload_meta()

            pdfs = sorted([p for p in UPLOAD_ORIGINAL_DIR.glob("*.pdf") if p.is_file()])
            if pdfs:
                st.caption("PDFï¼ˆoriginal/ï¼‰")
                for p in pdfs[:20]:
                    info = meta.get(p.name, {})
                    status = info.get("status", "unknown")
                    pages = info.get("pages", 0)
                    chars = info.get("chars", 0)
                    chunks = info.get("chunks", 0)
                    ex = extracted_json_path_for(p.name)
                    ex_ok = "OK" if ex.exists() and status == "ready" else "NG"
                    st.write(f"- {p.name}  / extracted: {ex_ok}  / {pages} pages  / {chunks} chunks  / {chars} chars")
                if len(pdfs) > 20:
                    st.caption(f"â€¦ã»ã‹ {len(pdfs)-20} ä»¶")
            else:
                st.caption("PDFã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚")

            ex_cnt = len(list(UPLOAD_EXTRACTED_DIR.glob('*.json')))
            st.caption(f"extracted/ : {ex_cnt} ä»¶")


        with st.expander("âš™ï¸ .env ç·¨é›†", expanded=False):
            st.caption(".env Path:")
            st.code(str(ENV_PATH))
            if ENV_PATH.exists():
                try:
                    t = ENV_PATH.read_text(encoding="utf-8", errors="ignore").strip()
                    if t:
                        st.caption("å†’é ­100æ–‡å­—")
                        st.write(t[:100])
                    else:
                        st.caption(".env ã¯ç©ºã§ã™ã€‚")
                except Exception as e:
                    st.caption(f"èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            else:
                st.caption(".env ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            if st.button(".env ã‚’ç·¨é›†", use_container_width=True):
                open_with_notepad(ENV_PATH)

        with st.expander("ğŸ”§ ç’°å¢ƒæƒ…å ±", expanded=False):
            st.write(f"[LLM] Base URL : `{LLM_BASE_URL}`")
            st.write(f"[LLM] Model    : `{LLM_MODEL}`")
            if use_remote_embedding():
                st.write("[EMB] Mode     : remote")
                st.write(f"[EMB] Base URL : `{EMB_BASE_URL}`")
                st.write(f"[EMB] Model    : `{EMB_MODEL}`")
            else:
                st.write("[EMB] Mode     : local MiniLM")
                st.write(f"[EMB] Path/ID  : `{LOCAL_EMB_MODEL_PATH}`")

    # -----------------------------------------------------
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤º
    # -----------------------------------------------------
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.write(m["content"])

    # -----------------------------------------------------
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    # -----------------------------------------------------
    query = st.chat_input("AIã®å›ç­”ã«ã¯èª¤ã‚ŠãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãã®ã¾ã¾ã§ã¯ãªãäº‹å®Ÿç¢ºèªã‚’è¡Œã£ã¦ã‹ã‚‰åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚")

    if query:
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.write(query)

        with st.spinner("ãƒŠãƒ¬ãƒƒã‚¸æ¤œç´¢ä¸­..."):
            try:
                # uploads ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯æœ€ä½1ä»¶æ··ãœã‚‹
                # è‘—è€…ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ç³»ã®è³ªå•ã¯PDF/ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”±æ¥ã‚’åšã‚ã«æ‹¾ã†ï¼ˆæ´¾ç”Ÿã‚¯ã‚¨ãƒªã‚‚ä½¿ã†ï¼‰
                qlower = (query or '').lower()
                is_author_q = ('è‘—è€…' in query) or ('ç­†è€…' in query) or ('author' in qlower) or ('authors' in qlower) or ('è«–æ–‡' in query)
                if is_author_q:
                    ctx = retrieve_with_embedding(query, top_k=10, min_uploads=5, use_variants=True)
                else:
                    ctx = retrieve_with_embedding(query, top_k=3, min_uploads=1, use_variants=False)
            except Exception as e:
                ctx = []
                st.error(f"Embedding ã‚¨ãƒ©ãƒ¼: {e}")

        if not ctx:
            answer = "ãƒŠãƒ¬ãƒƒã‚¸å†…ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        else:
            with st.spinner("LLM ã«å•ã„åˆã‚ã›ä¸­..."):
                answer = call_llm_with_context(query, ctx)

        with st.chat_message("assistant"):
            st.write(answer)
            if ctx:
                qlower2 = (query or "").lower()
                is_author_q2 = ("è‘—è€…" in query) or ("ç­†è€…" in query) or ("author" in qlower2) or ("authors" in qlower2) or ("è«–æ–‡" in query)
                if is_author_q2 and not any(str(c.get("source","")).startswith("uploads/") for c in ctx):
                    st.warning("uploads/PDF ãŒæ¤œç´¢å¯¾è±¡ã«å«ã¾ã‚Œã¦ã„ãªã„ã‹ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæœªæ§‹ç¯‰ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å·¦ãƒšã‚¤ãƒ³ã§PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ä¿å­˜ã—ã¦å–ã‚Šè¾¼ã‚€ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
                with st.expander("ğŸ” å‚ç…§ã—ãŸãƒ­ãƒ¼ã‚«ãƒ«ãƒŠãƒ¬ãƒƒã‚¸"):
                    for i, c in enumerate(ctx, 1):
                        st.markdown(f"**Doc {i}**  ï¼ˆscore={c.get('score', 0):.3f}ï¼‰")
                        st.caption(f"Source: {c.get('source','unknown')}")
                        st.write(c.get("text", ""))
            else:
                st.caption("è©²å½“ãƒŠãƒ¬ãƒƒã‚¸ãªã—ã€‚")
                st.caption(f"Index chunks: {doc_count} (uploads: {upload_count})")
                st.caption("â€» uploads/ ã«PDFãŒã‚ã‚‹ã®ã« uploads:0 ã®å ´åˆã¯ã€å·¦ãƒšã‚¤ãƒ³ã®ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

        add_history(query, answer)
        log_interaction(query, answer, ctx)

if __name__ == "__main__":
    main()
